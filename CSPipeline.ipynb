{"cells":[{"cell_type":"markdown","metadata":{"id":"LxK4lJg1Po3l"},"source":["### Use in Colab to resolve environment (otherwise ignore)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ctMiinQPPo3m"},"outputs":[],"source":["%%capture\n","!pip install pytorch-lightning\n","!pip install transformers\n","!pip install adapter-transformers\n","!pip install scikit-learn \n","!pip install datasets"]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"c0Lo2DU-QmoS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Module imports\n","import json\n","import pandas as pd\n","import torch\n","from sklearn.preprocessing import OneHotEncoder\n","from torch.utils.data import Dataset\n","from transformers import RobertaTokenizer\n","from datasets import load_dataset\n","import pytorch_lightning as pl\n","from transformers import AutoModel, AdamW, get_cosine_schedule_with_warmup\n","from transformers import RobertaForSequenceClassification, AutoAdapterModel\n","from torchmetrics.functional import f1_score, accuracy\n","import torch.nn as nn\n","import math\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"xwqt8aA6EoqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### HELPFULNESS Dataset"],"metadata":{"id":"A6vXZT91MlkT"}},{"cell_type":"markdown","source":["### IMDb Dataset"],"metadata":{"id":"tKxtAHZ9GlVJ"}},{"cell_type":"code","source":["imdb_dataset = load_dataset(\"imdb\")"],"metadata":{"id":"szUlrc8AGnzy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imdb_dataset"],"metadata":{"id":"p785vhyiHYe5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import RobertaTokenizer\n","# Tokenize dataset\n","model_name = \"roberta-base\"\n","tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","\n","def encode(examples):\n","    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=256)\n","\n","imdb_dataset = imdb_dataset.map(encode, batched=True)\n","imdb_dataset = imdb_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n","imdb_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"],"metadata":{"id":"u53D8W6NGoC2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imdb_dataset['validation'] = imdb_dataset['train'][20000:]\n","imdb_dataset['train'] = imdb_dataset['train'][:20000]\n"],"metadata":{"id":"2R1QYTqTJJyp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_imdb = torch.utils.data.Subset(imdb_dataset['train'], range(0, 20000))\n","validation_imdb = torch.utils.data.Subset(imdb_dataset['train'], range(0, 20000))"],"metadata":{"id":"pHiJy9oHKGKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imdb_dataset['train'] = train_imdb\n","imdb_dataset['validation'] = validation_imdb"],"metadata":{"id":"TR33DuMbL8S1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class IMDB_Classifier(pl.LightningModule):\n","\n","    def __init__(self, config: dict):\n","        super().__init__()\n","        self.config = config\n","        # 1. Static classification head is used for finetuning\n","        # self.pre_trained_model = RobertaForSequenceClassification.from_pretrained(config['model_name'], \n","        #                                                                           problem_type=\"multi_label_classification\", \n","        #                                                                           num_labels=self.config['n_labels'],\n","        #                                                                           return_dict = True)\n","        \n","\n","        # 2. AutoAdapterModel with adapters\n","        self.pre_trained_model = AutoAdapterModel.from_pretrained(config['model_name'])\n","        self.pre_trained_model.add_adapter(\"imdb\", config=\"pfeiffer\")\n","        self.pre_trained_model.add_classification_head(\"imdb\", \n","                                                       num_labels=self.config['n_labels'],\n","                                                       id2label={0:'negative',\n","                                                                 1:'positive'})\n","        # Enable adapter training \n","        self.pre_trained_model.set_active_adapters([\"imdb\"])   \n","        self.pre_trained_model.train_adapter(\"imdb\") \n","        self.softmax = nn.Softmax()\n","\n","\n","    def forward(self, input_ids, attention_mask, labels):\n","        # pre_trained model output\n","        # use this line when fine-tuning -- slight difference in input format\n","        # targets = torch.nn.functional.one_hot(labels, num_classes=3).double()\n","        logits = self.pre_trained_model(input_ids=input_ids, attention_mask=attention_mask).logits\n","        loss = self.pre_trained_model(input_ids=input_ids, labels=labels).loss\n","        output = self.softmax(logits)\n","        return loss, output\n","    \n","    def training_step(self, batch, batch_index):\n","        loss, output = self.forward(**batch)\n","        acc = accuracy(output, batch[\"labels\"])\n","        self.log(\"train_loss\", loss)\n","        self.log(\"train_acc_step\", acc)\n","        return {\"loss\": loss, \"accuracy\": acc, \"predictions\": output, \"labels\": batch[\"labels\"]}\n","\n","    \n","    def validation_step(self, batch, batch_index):\n","        loss, output = self.forward(**batch)\n","        f1 = f1_score(output, batch[\"labels\"])\n","        self.log(\"val_loss\", loss)\n","        self.log(\"val_f1_score\", f1)\n","        return {\"val_loss\": loss, \"f1_score\": f1, \"predictions\": output, \"labels\": batch[\"labels\"]}\n","\n","    def test_step(self, batch, batch_index):\n","        _, output = self.forward(**batch)\n","        f1 = f1_score(output, batch[\"labels\"])\n","        self.log(\"val_f1_score\", f1) \n","        return output\n","    \n","    def configure_optimizers(self):\n","        optimizer = AdamW(self.parameters(), lr=self.config['lr'], weight_decay=self.config['weight_decay'] ,no_deprecation_warning=True, correct_bias=False)\n","        total_steps = self.config['train_size'] / self.config['batch_size']\n","        warmup_steps = math.floor(total_steps * self.config['warmup'])\n","        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n","        return [optimizer], [scheduler]"],"metadata":{"id":"fpLRPE5yIr3t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","config = {\n","    # Some randomly typed-in initial configs\n","    'model_name': 'roberta-base',\n","    'batch_size': 32,\n","    'lr': 1e-4,\n","    'warmup': 0.06,\n","    'train_size': len(imdb_dataset['train']),\n","    'weight_decay': 0.000001,\n","    'n_epochs': 15,\n","    'n_labels': 2\n","}"],"metadata":{"id":"nPHbufT5GoIo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imdb_train_dl = torch.utils.data.DataLoader(imdb_dataset['train'], config['batch_size'])\n","imdb_val_dl = torch.utils.data.DataLoader(imdb_dataset['validation'], config['batch_size'])\n","\n","# model\n","imdb_model = IMDB_Classifier(config=config)\n","trainer = pl.Trainer(max_epochs=config['n_epochs'], gpus=1, num_sanity_val_steps=1, default_root_dir='/content/drive/MyDrive/imdb')\n","trainer.fit(imdb_model, imdb_train_dl, imdb_val_dl)"],"metadata":{"id":"VWcljx6AI5xM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%reload_ext tensorboard \n","%tensorboard --logdir ./drive/MyDrive/imdb/lightning_logs"],"metadata":{"id":"O4VkIPnyMqF7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### SciCite Dataset"],"metadata":{"id":"TsRmU9vkt0TX"}},{"cell_type":"code","source":["scicite_dataset = load_dataset(\"scicite\")"],"metadata":{"id":"JKrZRElMt7lb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import RobertaTokenizer\n","# Tokenize dataset\n","model_name = \"roberta-base\"\n","tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","\n","def encode(examples):\n","    return tokenizer(examples['string'], truncation=True, padding='max_length', max_length=80)\n","\n","scicite_dataset = scicite_dataset.map(encode, batched=True)\n","scicite_dataset = scicite_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n","scicite_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"],"metadata":{"id":"aAsfMTt3unox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sum(scicite_dataset['train']['labels']==2)"],"metadata":{"id":"lzMR33g-A52o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### SciCite Model&Training"],"metadata":{"id":"rctFNWkU16tc"}},{"cell_type":"code","source":["class Scicite_Classifier(pl.LightningModule):\n","\n","    def __init__(self, config: dict):\n","        super().__init__()\n","        self.config = config\n","        # 1. Static classification head is used for finetuning\n","        # self.pre_trained_model = RobertaForSequenceClassification.from_pretrained(config['model_name'], \n","        #                                                                           problem_type=\"multi_label_classification\", \n","        #                                                                           num_labels=self.config['n_labels'],\n","        #                                                                           return_dict = True)\n","        \n","\n","        # 2. AutoAdapterModel with adapters\n","        self.pre_trained_model = AutoAdapterModel.from_pretrained(config['model_name'])\n","        self.pre_trained_model.add_adapter(\"citation-intent\", config=\"parallel\")\n","        self.pre_trained_model.add_classification_head(\"citation-intent\", \n","                                                       num_labels=self.config['n_labels'],\n","                                                       id2label={0:'method',\n","                                                                 1:'background', \n","                                                                 2:'result'})\n","        # Enable adapter training \n","        self.pre_trained_model.set_active_adapters([\"citation-intent\"])   \n","        self.pre_trained_model.train_adapter(\"citation-intent\") \n","        self.softmax = nn.Softmax()\n","\n","\n","    def forward(self, input_ids, attention_mask, labels):\n","        # pre_trained model output\n","        # use this line when fine-tuning -- slight difference in input format\n","        # targets = torch.nn.functional.one_hot(labels, num_classes=3).double()\n","        logits = self.pre_trained_model(input_ids=input_ids, attention_mask=attention_mask).logits\n","        loss = self.pre_trained_model(input_ids=input_ids, labels=labels).loss\n","        output = self.softmax(logits)\n","        return loss, output\n","    \n","    def training_step(self, batch, batch_index):\n","        loss, output = self.forward(**batch)\n","        acc = accuracy(output, batch[\"labels\"])\n","        self.log(\"train_loss\", loss)\n","        self.log(\"train_acc_step\", acc)\n","        return {\"loss\": loss, \"accuracy\": acc, \"predictions\": output, \"labels\": batch[\"labels\"]}\n","\n","    \n","    def validation_step(self, batch, batch_index):\n","        loss, output = self.forward(**batch)\n","        f1 = f1_score(output, batch[\"labels\"])\n","        self.log(\"val_loss\", loss)\n","        self.log(\"val_f1_score\", f1)\n","        return {\"val_loss\": loss, \"f1_score\": f1, \"predictions\": output, \"labels\": batch[\"labels\"]}\n","\n","    def test_step(self, batch, batch_index):\n","        _, output = self.forward(**batch)\n","        f1 = f1_score(output, batch[\"labels\"])\n","        self.log(\"val_f1_score\", f1) \n","        return output\n","    \n","    def configure_optimizers(self):\n","        optimizer = AdamW(self.parameters(), lr=self.config['lr'], weight_decay=self.config['weight_decay'] ,no_deprecation_warning=True, correct_bias=False)\n","        total_steps = self.config['train_size'] / self.config['batch_size']\n","        warmup_steps = math.floor(total_steps * self.config['warmup'])\n","        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n","        return [optimizer], [scheduler]"],"metadata":{"id":"exdTZXGK1_MX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","config = {\n","    # Some randomly typed-in initial configs\n","    'model_name': 'roberta-base',\n","    'batch_size': 8,\n","    'lr': 5e-4,\n","    'warmup': 0.06,\n","    'train_size': len(scicite_dataset['train']),\n","    'weight_decay': 0.00001,\n","    'n_epochs': 15,\n","    'n_labels': 3\n","\n","}"],"metadata":{"id":"GxhNYgv63VV1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scicite_dataset['train']"],"metadata":{"id":"9HGDVhh341B4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scicite_train_dl = torch.utils.data.DataLoader(scicite_dataset['train'], config['batch_size'])\n","scicite_val_dl = torch.utils.data.DataLoader(scicite_dataset['validation'], config['batch_size'])\n","\n","# model\n","scicite_model = Scicite_Classifier(config=config)\n","trainer = pl.Trainer(max_epochs=config['n_epochs'], gpus=1, num_sanity_val_steps=1, default_root_dir='/content/drive/MyDrive/scicite')\n","trainer.fit(scicite_model, scicite_train_dl, scicite_val_dl)"],"metadata":{"id":"c36XUzEM8h-O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!kill 433"],"metadata":{"id":"khqjWbgYUEH_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%reload_ext tensorboard \n","%tensorboard --logdir ./drive/MyDrive/scicite/lightning_logs"],"metadata":{"id":"OhqPU1XIFkaJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","test_dataloaders = torch.utils.data.DataLoader(scicite_dataset['test'], config['batch_size'])\n","trainer.test(scicite_model ,dataloaders=test_dataloaders)"],"metadata":{"id":"TItyn1tKF3dh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"slBpl9gYPo3n"},"source":["### ACL-ARC Data Inspection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SEuA9ulhPo3o"},"outputs":[],"source":["# toy dataReader for exploration \n","class DataReader:\n","    def __init__(self, json_name, shuffle=False):\n","        with open(json_name, 'r') as json_file:\n","            raw_json = list(json_file)\n","        self.raw = raw_json\n","        self.raw_objects = []\n","        for item in self.raw:\n","            self.raw_objects.append(json.loads(item))\n","        self.df = pd.DataFrame(self.raw_objects)\n","\n","\n","    def get_stats(self):   \n","        return self.df.head()\n","\n","    def get_data(self):\n","        # import IPython; IPython.embed(); exit(1)\n","        return self.df['text'], self.df['intent']"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"vkMKY7oPRH3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOOAEYQJPo3o"},"outputs":[],"source":["train_path = \"./drive/MyDrive/DL project/project/acl-arc/train.jsonl\"\n","val_path = \"./drive/MyDrive/DL project/project/acl-arc/dev.jsonl\"\n","test_path = \"./drive/MyDrive/DL project/project/acl-arc/test.jsonl\"\n","train_acl = DataReader(json_name=train_path).df\n","test_acl = DataReader(json_name=val_path).df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"86OFTdLzPo3p"},"outputs":[],"source":["train_acl.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJJ99XM4Po3q"},"outputs":[],"source":["# Simple label view\n","labels = list(set(train_acl['intent']))\n","train_acl.groupby('intent').count()['text'].plot.bar()"]},{"cell_type":"code","source":["attributes = ['Background', 'CompareOrContrast', 'Extends', 'Future', 'Motivation', 'Uses']"],"metadata":{"id":"VbWP9e8HL5gw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1RDbUfgkPo3q"},"source":["### ACL-ARC Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMDBLve1Po3r"},"outputs":[],"source":["class ACL_Dataset(Dataset):\n","    def __init__(self, data_path, tokenizer, attributes, max_token_len=128):\n","        self.data_path = data_path\n","        self.tokenizer = tokenizer\n","        self.attribute = attributes\n","        self.max_token_len = max_token_len\n","        self._prepare_data()\n","\n","    def _prepare_data(self):\n","        '''\n","        Place to add other data preparations (sampling / train&test separation)\n","\n","        '''\n","        if self.data_path[-3:] != \"csv\":\n","            with open(self.data_path, 'r') as json_file:\n","                raw_json = list(json_file)\n","            raw_objects = []\n","            for item in raw_json:\n","                raw_objects.append(json.loads(item))\n","            self.data = pd.DataFrame(raw_objects)\n","\n","            # Turn into one-hot encoding\n","            encoder = OneHotEncoder(handle_unknown='ignore')\n","            encoder_df = pd.DataFrame(encoder.fit_transform(self.data[['intent']]).toarray())\n","            encoder_df.columns = self.attribute\n","            self.data = self.data.join(encoder_df)\n","\n","        else:\n","            print(\"Not yet implemented for csv\")\n","\n","    def __len__ (self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        '''\n","            samples and labels loaded here \n","        '''\n","        item = self.data.iloc[index]\n","        labels = torch.Tensor(item[self.attribute])\n","        text = str(item.cleaned_cite_text)\n","        tokens = self.tokenizer.encode_plus(text, add_special_tokens=True, \n","                    return_tensors='pt', truncation=True, max_length = self.max_token_len, \n","                    padding=\"max_length\", return_attention_mask=True)\n","    \n","        return {\"input_ids\": tokens.input_ids.flatten(), \"attention_mask\": tokens.attention_mask.flatten(), \"labels\": labels}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AT5JQdjgPo3s"},"outputs":[],"source":["model_name = \"roberta-base\"\n","tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","train = ACL_Dataset(train_path, tokenizer, attributes=attributes)\n"]},{"cell_type":"code","source":["train.data[['intent', 'CompareOrContrast', 'Uses', 'Background', 'Future', 'Motivation', 'Extends']]"],"metadata":{"id":"t1Z03YFJMvrh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ivX7NKc1Po3s"},"source":["### ACL-ARC Data module"]},{"cell_type":"code","source":["import pytorch_lightning as pl\n","from torch.utils.data import DataLoader"],"metadata":{"id":"9PzMjO4U2GQQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jxpxS7JFPo3t"},"outputs":[],"source":["class ACL_DataLoader(pl.LightningDataModule):\n","    def __init__(self, train_path, val_path, test_path, attributes, batch_size:int = 32, max_token_length: int = 128, model_name = \"roberta-base\"):\n","        super().__init__()\n","        self.train_path = train_path\n","        self.val_path = val_path\n","        self.batch_size = batch_size\n","        self.max_token_length = max_token_length\n","        self.model_name = model_name\n","        self.attributes = attributes\n","        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","\n","\n","    def setup(self, stage=None):\n","        if stage in (None, \"fit\"):\n","            self.train_dataset = ACL_Dataset(train_path, self.tokenizer, attributes=self.attributes)\n","            self.val_dataset = ACL_Dataset(val_path, self.tokenizer, attributes=self.attributes)\n","\n","        if stage == \"predict\":\n","            self.val_dataset = ACL_Dataset(val_path, self.tokenizer, attributes=self.attributes)\n","    \n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers = 4, shuffle=True)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers = 4, shuffle=False)\n","\n","    def predict_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers = 4, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hWgtq6vmPo3t"},"outputs":[],"source":["acl_datamodule = ACL_DataLoader(train_path=train_path, val_path=val_path, test_path=test_path, attributes=attributes)\n","acl_datamodule.setup()\n","acl_dataloader = acl_datamodule.train_dataloader()"]},{"cell_type":"markdown","metadata":{"id":"qVcLwZK9Po3u"},"source":["### ACL-ARC Model"]},{"cell_type":"code","source":["attributes"],"metadata":{"id":"NzsWCa6gL0WW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EtmAK9XlPo3u"},"outputs":[],"source":["from transformers import AutoModel, AdamW, get_cosine_schedule_with_warmup\n","from transformers import RobertaForSequenceClassification, AutoAdapterModel\n","from torchmetrics.functional import f1_score, accuracy\n","import torch.nn as nn\n","import math\n","\n","class ACL_Classifier(pl.LightningModule):\n","\n","    def __init__(self, config: dict):\n","        super().__init__()\n","        self.config = config\n","\n","        # Static classification head is used for finetuning\n","        # self.pre_trained_model = RobertaForSequenceClassification.from_pretrained(config['model_name'], \n","        #                                                                           problem_type=\"multi_label_classification\", \n","        #                                                                           num_labels=self.config['n_labels'],\n","        #                                                                           return_dict = True)\n","        \n","\n","        # May switch to dynamic ones \n","        self.pre_trained_model = AutoAdapterModel.from_pretrained(config['model_name'])\n","        self.pre_trained_model.add_adapter(\"citation-intent\", config=\"parallel\")\n","        self.pre_trained_model.add_classification_head(\"citation-intent\", \n","                                                       num_labels=6,\n","                                                       id2label={0:'Background',\n","                                                                 1:'CompareOrContrast', \n","                                                                 2:'Extends', \n","                                                                 3:'Future', \n","                                                                 4:'Motivation', \n","                                                                 5:'Uses'})\n","\n","        # Enable adapter training \n","        self.pre_trained_model.set_active_adapters([\"citation-intent\"])   \n","        self.pre_trained_model.train_adapter(\"citation-intent\") \n","        self.softmax = nn.Softmax()\n","\n","\n","    def forward(self, input_ids, attention_mask, labels):\n","        # pre_trained model output\n","        target = torch.argmax(labels, axis=1)\n","        logits = self.pre_trained_model(input_ids=input_ids, attention_mask=attention_mask).logits\n","        loss = self.pre_trained_model(input_ids=input_ids, labels=target).loss\n","        output = self.softmax(logits)\n","        return loss, output\n","    \n","    def training_step(self, batch, batch_index):\n","        loss, output = self.forward(**batch)\n","        target = torch.argmax(batch[\"labels\"], axis=1)\n","        acc = accuracy(output, target)\n","        self.log(\"train_loss\", loss)\n","        self.log(\"train_acc_step\", acc)\n","        return {\"loss\": loss, \"accuracy\": acc, \"predictions\": output, \"labels\": batch[\"labels\"]}\n","\n","    \n","    def validation_step(self, batch, batch_index):\n","        loss, output = self.forward(**batch)\n","        target = torch.argmax(batch[\"labels\"], axis=1)\n","        f1 = f1_score(output, target, average=\"macro\", num_classes=6)\n","        self.log(\"val_loss\", loss)\n","        self.log(\"val_f1_score\", f1)\n","        return {\"val_loss\": loss, \"f1_score\": f1, \"predictions\": output, \"labels\": batch[\"labels\"]}\n","\n","    def test_step(self, batch, batch_index):\n","        _, output = self.forward(**batch)\n","        target = torch.argmax(batch[\"labels\"], axis=1)\n","        # f1 = f1_score(output, target, average=\"macro\", num_classes=6)\n","        f1 = f1_score(output, target)\n","        self.log(\"val_f1_score\", f1) \n","        return output\n","    \n","    def configure_optimizers(self):\n","        optimizer = AdamW(self.parameters(), lr=self.config['lr'], weight_decay=self.config['weight_decay'] ,no_deprecation_warning=True, correct_bias=False)\n","        total_steps = self.config['train_size'] / self.config['batch_size']\n","        warmup_steps = math.floor(total_steps * self.config['warmup'])\n","        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n","        return [optimizer], [scheduler]"]},{"cell_type":"code","source":["\n","config = {\n","    # Some randomly typed-in initial configs\n","    'model_name': 'roberta-base',\n","    'batch_size': 32,\n","    'lr': 1e-4,\n","    # 1e-4\n","    'warmup': 0.06,\n","    'weight_decay': 0.00001,\n","    'n_epochs': 30,\n","    'train_size': len(acl_datamodule.train_dataloader()),\n","    'n_labels': len(labels)\n","\n","\n","}"],"metadata":{"id":"5_MhnZcwtsjh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u31rZ2MGPo3v"},"outputs":[],"source":["'''Single output sanity check'''\n","model = ACL_Classifier(config=config)\n","idx = 0\n","input_ids = train.__getitem__(idx)['input_ids']\n","attention_mask = train.__getitem__(idx)['attention_mask']\n","labels = train.__getitem__(idx)['labels']\n","loss, output = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0), labels.unsqueeze(0))\n","print(\"loss\" + str(loss))\n","print(\"raw prediction: \" + str(output))\n","print(\"label: \" + str(labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5t-7Ar6Po3v"},"outputs":[],"source":["# Copy datamodule here - for convenience\n","acl_datamodule = ACL_DataLoader(train_path=train_path, val_path=val_path, test_path=test_path, attributes=attributes, batch_size=config['batch_size'])\n","acl_datamodule.setup()\n","model = ACL_Classifier(config=config)\n","\n","# Auto-Training loop\n","trainer = pl.Trainer(max_epochs=config['n_epochs'], gpus=1, num_sanity_val_steps=1, default_root_dir='/content/drive/MyDrive/aclarc')\n","trainer.fit(model, acl_datamodule)"]},{"cell_type":"code","source":["%reload_ext tensorboard\n","%tensorboard --logdir ./drive/MyDrive/aclarc/lightning_logs/"],"metadata":{"id":"2YcihZKJbCfv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","test_dataloaders = acl_datamodule.predict_dataloader()\n","trainer.test(model ,dataloaders=test_dataloaders)"],"metadata":{"id":"ngielPnpeqgn"},"execution_count":null,"outputs":[]}],"metadata":{"interpreter":{"hash":"9cadeb9d479b9b5e991836f2ad75dbb05fb6b50757eddf2eabcc5232d77dbf5e"},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4,"colab":{"name":"pipeline.ipynb","private_outputs":true,"provenance":[],"machine_shape":"hm","collapsed_sections":["tKxtAHZ9GlVJ","TsRmU9vkt0TX","slBpl9gYPo3n","1RDbUfgkPo3q","ivX7NKc1Po3s"]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}