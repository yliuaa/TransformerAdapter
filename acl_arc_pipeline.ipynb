{"cells":[{"cell_type":"markdown","metadata":{"id":"uKVPfcnxPo3h"},"source":["## Data preparation and settings"]},{"cell_type":"markdown","metadata":{"id":"LxK4lJg1Po3l"},"source":["### Use in Colab to resolve environment (otherwise ignore)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ctMiinQPPo3m"},"outputs":[],"source":["%%capture\n","!pip install pytorch-lightning\n","!pip install transformers\n","!pip install adapter-transformers\n","!pip install scikit-learn "]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"c0Lo2DU-QmoS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"slBpl9gYPo3n"},"source":["### Data Inspection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iacw5JQyPo3n"},"outputs":[],"source":["import json\n","import pandas as pd\n","import torch\n","from sklearn.preprocessing import OneHotEncoder\n","from torch.utils.data import Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SEuA9ulhPo3o"},"outputs":[],"source":["# toy dataReader for exploration \n","class DataReader:\n","    def __init__(self, json_name, shuffle=False):\n","        with open(json_name, 'r') as json_file:\n","            raw_json = list(json_file)\n","        self.raw = raw_json\n","        self.raw_objects = []\n","        for item in self.raw:\n","            self.raw_objects.append(json.loads(item))\n","        self.df = pd.DataFrame(self.raw_objects)\n","\n","\n","    def get_stats(self):   \n","        return self.df.head()\n","\n","    def get_data(self):\n","        # import IPython; IPython.embed(); exit(1)\n","        return self.df['text'], self.df['intent']"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"vkMKY7oPRH3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOOAEYQJPo3o"},"outputs":[],"source":["train_path = \"./drive/MyDrive/DL project/project/acl-arc/train.jsonl\"\n","val_path = \"./drive/MyDrive/DL project/project/acl-arc/dev.jsonl\"\n","test_path = \"./drive/MyDrive/DL project/project/acl-arc/test.jsonl\"\n","train_acl = DataReader(json_name=train_path).df\n","test_acl = DataReader(json_name=val_path).df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"86OFTdLzPo3p"},"outputs":[],"source":["train_acl.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJJ99XM4Po3q"},"outputs":[],"source":["# Simple label view\n","labels = list(set(train_acl['intent']))\n","train_acl.groupby('intent').count()['text'].plot.bar()"]},{"cell_type":"code","source":["attributes = ['Background', 'CompareOrContrast', 'Extends', 'Future', 'Motivation', 'Uses']"],"metadata":{"id":"VbWP9e8HL5gw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1RDbUfgkPo3q"},"source":["### Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMDBLve1Po3r"},"outputs":[],"source":["class ACL_Dataset(Dataset):\n","    def __init__(self, data_path, tokenizer, attributes, max_token_len=128):\n","        self.data_path = data_path\n","        self.tokenizer = tokenizer\n","        self.attribute = attributes\n","        self.max_token_len = max_token_len\n","        self._prepare_data()\n","\n","    def _prepare_data(self):\n","        '''\n","        Place to add other data preparations (sampling / train&test separation)\n","\n","        '''\n","        if self.data_path[-3:] != \"csv\":\n","            with open(self.data_path, 'r') as json_file:\n","                raw_json = list(json_file)\n","            raw_objects = []\n","            for item in raw_json:\n","                raw_objects.append(json.loads(item))\n","            self.data = pd.DataFrame(raw_objects)\n","\n","            # Turn into one-hot encoding\n","            encoder = OneHotEncoder(handle_unknown='ignore')\n","            encoder_df = pd.DataFrame(encoder.fit_transform(self.data[['intent']]).toarray())\n","            encoder_df.columns = self.attribute\n","            self.data = self.data.join(encoder_df)\n","\n","        else:\n","            print(\"Not yet implemented for csv\")\n","\n","    def __len__ (self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        '''\n","            samples and labels loaded here \n","        '''\n","        item = self.data.iloc[index]\n","        labels = torch.Tensor(item[self.attribute])\n","        text = str(item.cleaned_cite_text)\n","        tokens = self.tokenizer.encode_plus(text, add_special_tokens=True, \n","                    return_tensors='pt', truncation=True, max_length = self.max_token_len, \n","                    padding=\"max_length\", return_attention_mask=True)\n","    \n","        return {\"input_ids\": tokens.input_ids.flatten(), \"attention_mask\": tokens.attention_mask.flatten(), \"labels\": labels}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AT5JQdjgPo3s"},"outputs":[],"source":["from transformers import RobertaTokenizer\n","model_name = \"roberta-base\"\n","tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","train = ACL_Dataset(train_path, tokenizer, attributes=attributes)\n"]},{"cell_type":"code","source":["train.data[['intent', 'CompareOrContrast', 'Uses', 'Background', 'Future', 'Motivation', 'Extends']]"],"metadata":{"id":"t1Z03YFJMvrh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ivX7NKc1Po3s"},"source":["### Data module"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZQ63fRUPo3s"},"outputs":[],"source":["import pytorch_lightning as pl\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jxpxS7JFPo3t"},"outputs":[],"source":["class ACL_DataLoader(pl.LightningDataModule):\n","    def __init__(self, train_path, val_path, test_path, attributes, batch_size:int = 32, max_token_length: int = 128, model_name = \"roberta-base\"):\n","        super().__init__()\n","        self.train_path = train_path\n","        self.val_path = val_path\n","        self.batch_size = batch_size\n","        self.max_token_length = max_token_length\n","        self.model_name = model_name\n","        self.attributes = attributes\n","        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","\n","\n","    def setup(self, stage=None):\n","        if stage in (None, \"fit\"):\n","            self.train_dataset = ACL_Dataset(train_path, self.tokenizer, attributes=self.attributes)\n","            self.val_dataset = ACL_Dataset(val_path, self.tokenizer, attributes=self.attributes)\n","\n","        if stage == \"predict\":\n","            self.val_dataset = ACL_Dataset(val_path, self.tokenizer, attributes=self.attributes)\n","    \n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers = 4, shuffle=True)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers = 4, shuffle=False)\n","\n","    def predict_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers = 4, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hWgtq6vmPo3t"},"outputs":[],"source":["acl_datamodule = ACL_DataLoader(train_path=train_path, val_path=val_path, test_path=test_path, attributes=attributes)\n","acl_datamodule.setup()\n","acl_dataloader = acl_datamodule.train_dataloader()"]},{"cell_type":"markdown","metadata":{"id":"qVcLwZK9Po3u"},"source":["### Model"]},{"cell_type":"code","source":["attributes"],"metadata":{"id":"NzsWCa6gL0WW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EtmAK9XlPo3u"},"outputs":[],"source":["from transformers import AutoModel, AdamW, get_cosine_schedule_with_warmup\n","from transformers import RobertaForSequenceClassification, RobertaModelWithHeads\n","from torchmetrics.functional import f1_score, accuracy\n","import torch.nn as nn\n","import math\n","\n","'''\n","TODO change model accordingly\n","'''\n","class ACL_Classifier(pl.LightningModule):\n","\n","    def __init__(self, config: dict, adapter_config: dict):\n","        super().__init__()\n","        self.config = config\n","\n","        # Static classification head is used\n","        self.pre_trained_model = RobertaForSequenceClassification.from_pretrained(config['model_name'], \n","                                                                                  problem_type=\"multi_label_classification\", \n","                                                                                  num_labels=self.config['n_labels'],\n","                                                                                  return_dict = True)\n","        \n","        # May switch to dynamic ones \n","        # self.pre_trained_model = RobertaModelWithHeads.from_pretrained(config = config)\n","        # self.pre_trained_model.add_adapter(\"citation-intent\", config=adapter_config)\n","\n","        # # TODO change the parameters\n","        # # model.add_classification_head(\n","        # #     \"rotten_tomatoes\",\n","        # #     num_labels=2,\n","        # #     id2label={ 0: \"üëé\", 1: \"üëç\"}\n","        # # )\n","\n","        # # Enable adapter training \n","        # self.pre_trained_model.train_adapter(\"citation-intent\") \n","        # self.pre_trained_model.set_active_adapters([\"citation-intent\"])   \n","        self.softmax = nn.Softmax()\n","\n","\n","    def forward(self, input_ids, attention_mask, labels):\n","        # pre_trained model output\n","        logits = self.pre_trained_model(input_ids=input_ids, attention_mask=attention_mask).logits\n","        loss = self.pre_trained_model(input_ids=input_ids, labels=labels).loss\n","        output = self.softmax(logits)\n","        return loss, output\n","    \n","    def training_step(self, batch, batch_index):\n","        loss, output = self.forward(**batch)\n","        target = torch.argmax(batch[\"labels\"], axis=1)\n","        acc = accuracy(output, target)\n","        self.log(\"train_loss\", loss)\n","        self.log(\"train_acc_step\", acc)\n","        return {\"loss\": loss, \"accuracy\": acc, \"predictions\": output, \"labels\": batch[\"labels\"]}\n","\n","    \n","    def validation_step(self, batch, batch_index):\n","        loss, output = self.forward(**batch)\n","        target = torch.argmax(batch[\"labels\"], axis=1)\n","        f1 = f1_score(output, target)\n","        self.log(\"val_loss\", loss)\n","        self.log(\"val_f1_score\", f1)\n","        return {\"val_loss\": loss, \"f1_score\": f1, \"predictions\": output, \"labels\": batch[\"labels\"]}\n","\n","    def test_step(self, batch, batch_index):\n","        _, output = self.forward(**batch)\n","        target = torch.argmax(batch[\"labels\"], axis=1)\n","        f1 = f1_score(output, target)\n","        self.log(\"val_f1_score\", f1) \n","        return output\n","    \n","    def configure_optimizers(self):\n","        optimizer = AdamW(self.parameters(), lr=self.config['lr'], weight_decay=self.config['weight_decay'] ,no_deprecation_warning=True, correct_bias=False)\n","        total_steps = self.config['train_size'] / self.config['batch_size']\n","        warmup_steps = math.floor(total_steps * self.config['warmup'])\n","        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n","        return [optimizer], [scheduler]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aubfIew8Po3v"},"outputs":[],"source":["from transformers import HoulsbyConfig\n","\n","adapter_config = HoulsbyConfig()\n","\n","roberta_config = {\n","    # Some randomly typed-in initial configs\n","    'model_name': 'roberta-base',\n","    'batch_size': 16,\n","    'lr': 2e-5,\n","    'warmup': 0.06,\n","    'weight_decay': 0.01,\n","    'n_epochs': 10,\n","    'train_size': len(acl_datamodule.train_dataloader()),\n","    'n_labels': len(labels)\n","\n","}\n","\n","adapter_config\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u31rZ2MGPo3v"},"outputs":[],"source":["'''Single output sanity check'''\n","model = ACL_Classifier(config=roberta_config, adapter_config=adapter_config)\n","idx = 0\n","input_ids = train.__getitem__(idx)['input_ids']\n","attention_mask = train.__getitem__(idx)['attention_mask']\n","labels = train.__getitem__(idx)['labels']\n","loss, output = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0), labels.unsqueeze(0))\n","print(\"loss\" + str(loss))\n","print(\"raw prediction: \" + str(output))\n","print(\"label: \" + str(labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5t-7Ar6Po3v"},"outputs":[],"source":["# Copy datamodule here - for convenience\n","acl_datamodule = ACL_DataLoader(train_path=train_path, val_path=val_path, test_path=test_path, attributes=attributes, batch_size=roberta_config['batch_size'])\n","acl_datamodule.setup()\n","model = ACL_Classifier(config=roberta_config, adapter_config=adapter_config)\n","\n","# Auto-Training loop\n","trainer = pl.Trainer(max_epochs=roberta_config['n_epochs'], gpus=1, num_sanity_val_steps=1, default_root_dir='/content/drive/MyDrive')\n","trainer.fit(model, acl_datamodule)"]},{"cell_type":"code","source":["%reload_ext tensorboard\n","%tensorboard --logdir ./drive/MyDrive/lightning_logs"],"metadata":{"id":"2YcihZKJbCfv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation"],"metadata":{"id":"_Gd_-z8Zefae"}},{"cell_type":"code","source":["# saved_model = ACL_Classifier.load_from_checkpoint('./drive/MyDrive/lightning_logs/version_4/checkpoints/saved.ckpt', config=roberta_config, adapter_config=adapter_config)"],"metadata":{"id":"wjwotk_k5DMK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","test_dataloaders = acl_datamodule.predict_dataloader()\n","trainer.test(model ,dataloaders=test_dataloaders)\n","\n"],"metadata":{"id":"ngielPnpeqgn"},"execution_count":null,"outputs":[]}],"metadata":{"interpreter":{"hash":"9cadeb9d479b9b5e991836f2ad75dbb05fb6b50757eddf2eabcc5232d77dbf5e"},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4,"colab":{"name":"acl_arc_pipeline.ipynb","private_outputs":true,"provenance":[],"machine_shape":"hm","collapsed_sections":["LxK4lJg1Po3l","slBpl9gYPo3n","1RDbUfgkPo3q","ivX7NKc1Po3s"]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}