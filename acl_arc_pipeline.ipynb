{"cells":[{"cell_type":"markdown","metadata":{"id":"uKVPfcnxPo3h"},"source":["## Data preparation and settings"]},{"cell_type":"markdown","metadata":{"id":"LxK4lJg1Po3l"},"source":["### Use in Colab to resolve environment (otherwise ignore)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ctMiinQPPo3m"},"outputs":[],"source":["%%capture\n","!pip install pytorch-lightning\n","!pip install transformers\n","!pip install adapter-transformers\n","!pip install scikit-learn "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c0Lo2DU-QmoS"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"slBpl9gYPo3n"},"source":["### Data Inspection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iacw5JQyPo3n"},"outputs":[],"source":["import json\n","import pandas as pd\n","import torch\n","from sklearn.preprocessing import OneHotEncoder\n","from torch.utils.data import Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SEuA9ulhPo3o"},"outputs":[],"source":["# toy dataReader for exploration \n","class DataReader:\n","    def __init__(self, json_name, shuffle=False):\n","        with open(json_name, 'r') as json_file:\n","            raw_json = list(json_file)\n","        self.raw = raw_json\n","        self.raw_objects = []\n","        for item in self.raw:\n","            self.raw_objects.append(json.loads(item))\n","        self.df = pd.DataFrame(self.raw_objects)\n","\n","\n","    def get_stats(self):   \n","        return self.df.head()\n","\n","    def get_data(self):\n","        # import IPython; IPython.embed(); exit(1)\n","        return self.df['text'], self.df['intent']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkMKY7oPRH3-"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOOAEYQJPo3o"},"outputs":[],"source":["train_path = \"./drive/MyDrive/DL project/project/acl-arc/train.jsonl\"\n","val_path = \"./drive/MyDrive/DL project/project/acl-arc/test.jsonl\"\n","train_acl = DataReader(json_name=train_path).df\n","test_acl = DataReader(json_name=val_path).df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"86OFTdLzPo3p"},"outputs":[],"source":["train_acl.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJJ99XM4Po3q"},"outputs":[],"source":["# Simple label view\n","labels = list(set(train_acl['intent']))\n","train_acl.groupby('intent').count()['text'].plot.bar()"]},{"cell_type":"markdown","metadata":{"id":"1RDbUfgkPo3q"},"source":["### Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMDBLve1Po3r"},"outputs":[],"source":["class ACL_Dataset(Dataset):\n","    def __init__(self, data_path, tokenizer, max_token_len=128):\n","        self.data_path = data_path\n","        self.tokenizer = tokenizer\n","        self.attribute = None\n","        self.max_token_len = max_token_len\n","        self._prepare_data()\n","\n","    def _prepare_data(self):\n","        '''\n","        Place to add other data preparations (sampling / train&test separation)\n","\n","        '''\n","        if self.data_path[-3:] != \"csv\":\n","            with open(self.data_path, 'r') as json_file:\n","                raw_json = list(json_file)\n","            raw_objects = []\n","            for item in raw_json:\n","                raw_objects.append(json.loads(item))\n","            self.data = pd.DataFrame(raw_objects)\n","\n","            # Turn into one-hot encoding\n","            encoder = OneHotEncoder(handle_unknown='ignore')\n","            encoder_df = pd.DataFrame(encoder.fit_transform(self.data[['intent']]).toarray())\n","            self.attribute = list(set(train_acl['intent']))\n","            encoder_df.columns = self.attribute\n","            self.data = self.data.join(encoder_df)\n","\n","        else:\n","            print(\"Not yet implemented for csv\")\n","\n","    def __len__ (self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        # TODO get desired items by index\n","        item = self.data.iloc[index]\n","        labels = torch.Tensor(item[self.attribute])\n","        text = str(item.cleaned_cite_text)\n","        tokens = self.tokenizer.encode_plus(text, add_special_tokens=True, \n","                    return_tensors='pt', truncation=True, max_length = self.max_token_len, \n","                    padding=\"max_length\", return_attention_mask=True)\n","    \n","        return {\"input_ids\": tokens.input_ids.flatten(), \"attention_mask\": tokens.attention_mask.flatten(), \"labels\": labels}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AT5JQdjgPo3s"},"outputs":[],"source":["from transformers import AutoTokenizer\n","model_name = \"roberta-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","train = ACL_Dataset(train_path, tokenizer)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gcVKozdtPo3s"},"outputs":[],"source":["# train.__getitem__(0)"]},{"cell_type":"markdown","metadata":{"id":"ivX7NKc1Po3s"},"source":["### Data module"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZQ63fRUPo3s"},"outputs":[],"source":["import pytorch_lightning as pl\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jxpxS7JFPo3t"},"outputs":[],"source":["class ACL_DataLoader(pl.LightningDataModule):\n","    def __init__(self, train_path, val_path, batch_size:int = 32, max_token_length: int = 128, model_name = \"roberta-base\"):\n","        super().__init__()\n","        self.train_path = train_path\n","        self.val_path = val_path\n","        self.batch_size = batch_size\n","        self.max_token_length = max_token_length\n","        self.model_name = model_name\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","\n","    def setup(self, stage=None):\n","        if stage in (None, \"fit\"):\n","            self.train_dataset = ACL_Dataset(train_path, self.tokenizer)\n","            self.val_dataset = ACL_Dataset(val_path, self.tokenizer)\n","            self.attributes = self.val_dataset.attribute\n","\n","        if stage == \"predict\":\n","            self.val_dataset = ACL_Dataset(val_path, self.tokenizer)\n","    \n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers = 4, shuffle=False)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers = 4, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hWgtq6vmPo3t"},"outputs":[],"source":["acl_datamodule = ACL_DataLoader(train_path=train_path, val_path=val_path)\n","acl_datamodule.setup()\n","acl_dataloader = acl_datamodule.train_dataloader()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a0NETDSfPo3u"},"outputs":[],"source":["# number of batches \n","len(acl_dataloader)"]},{"cell_type":"markdown","metadata":{"id":"qVcLwZK9Po3u"},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EtmAK9XlPo3u"},"outputs":[],"source":["from transformers import AutoModel, AdamW, get_cosine_schedule_with_warmup\n","import torch.nn as nn\n","import math\n","from torchmetrics.functional.classification import f1_score, auroc\n","\n","'''\n","TODO change model accordingly\n","'''\n","class ACL_Classifier(pl.LightningModule):\n","\n","    def __init__(self, config: dict):\n","        super().__init__()\n","        self.config = config\n","        self.pre_trained_model = AutoModel.from_pretrained(config['model_name'], return_dict = True)\n","        # TODO Adapters to be added when base is trained \n","        self.pre_trained_adapter = None\n","        self.hidden = nn.Linear(self.pre_trained_model.config.hidden_size, self.pre_trained_model.config.hidden_size)\n","        self.classification = nn.Linear(self.pre_trained_model.config.hidden_size, self.config['n_labels'])\n","\n","        # Initialization module -- xavier or some others?\n","        torch.nn.init.xavier_uniform_(self.hidden.weight)\n","        torch.nn.init.xavier_uniform_(self.classification.weight)\n","    \n","        # others \n","        self.relu = nn.ReLU()\n","        self.loss_func = nn.BCEWithLogitsLoss(reduction=\"mean\")\n","        self.dropout = nn.Dropout()\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        # pre_trained model output\n","        output = self.pre_trained_model(input_ids=input_ids, attention_mask=attention_mask)\n","        output = torch.mean(output.last_hidden_state, 1)   # mean pooling as in the paper \n","        # FF classifier \n","        output = self.hidden(output)\n","        # output = self.dropout(output)\n","        output = self.relu(output)\n","        output = self.classification(output)\n","        # loss (change accordingly with the type of loss function used)\n","        loss = 0\n","        if labels is not None:\n","            loss = self.loss_func(output.view(-1, self.config['n_labels']), labels.view(-1, self.config['n_labels']))\n","        return loss, output\n","    \n","    def training_step(self, batch, batch_index):\n","        loss, output = self.forward(**batch)\n","        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n","        return {\"loss\": loss, \"predictions\": output, \"labels\": batch[\"labels\"]}\n","\n","    \n","    def validation_step(self, batch, batch_index):\n","        loss, output = self.forward(**batch)\n","        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n","        return {\"val_loss\": loss, \"predictions\": output, \"labels\": batch[\"labels\"]}\n","\n","    def predict_step(self, batch, batch_index):\n","        _, output = self.forward(**batch)\n","        return output\n","    \n","    def configure_optimizers(self):\n","        optimizer = AdamW(self.parameters(), lr=self.config['lr'], weight_decay=self.config['weight_decay'], no_deprecation_warning=True, correct_bias=False)\n","        total_steps = self.config['train_size'] / self.config['batch_size']\n","        warmup_steps = math.floor(total_steps * self.config['warmup'])\n","        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n","        return [optimizer], [scheduler]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aubfIew8Po3v"},"outputs":[],"source":["config = {\n","    # Some randomly typed-in initial configs\n","    'model_name': 'roberta-base',\n","    'batch_size': 256,\n","    'lr': 1e-4,\n","    'warmup': 0.06,\n","    'weight_decay': 0.01,\n","    'n_epochs': 100,\n","    'train_size': len(acl_datamodule.train_dataloader()),\n","    'n_labels': len(labels)\n","\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u31rZ2MGPo3v"},"outputs":[],"source":["'''Single output sanity check'''\n","model = ACL_Classifier(config=config)\n","idx = 0\n","input_ids = train.__getitem__(idx)['input_ids']\n","attention_mask = train.__getitem__(idx)['attention_mask']\n","labels = train.__getitem__(idx)['labels']\n","loss, output = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0), labels.unsqueeze(0))\n","print(\"loss\" + str(loss))\n","print(\"raw prediction: \" + str(output))\n","\n","print(\"label: \" + str(labels))\n","print(train.__getitem__(idx)['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5t-7Ar6Po3v"},"outputs":[],"source":["# Copy datamodule here - for convenience\n","acl_datamodule = ACL_DataLoader(train_path=train_path, val_path=val_path, batch_size=config['batch_size'])\n","acl_datamodule.setup()\n","model = ACL_Classifier(config=config)\n","\n","trainer = pl.Trainer(max_epochs=config['n_epochs'], gpus=1, num_sanity_val_steps=10)\n","trainer.fit(model, acl_datamodule)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2YcihZKJbCfv"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir ./lightning_logs/"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["LxK4lJg1Po3l","slBpl9gYPo3n","1RDbUfgkPo3q","ivX7NKc1Po3s"],"machine_shape":"hm","name":"acl_arc_pipeline.ipynb","private_outputs":true,"provenance":[]},"interpreter":{"hash":"9cadeb9d479b9b5e991836f2ad75dbb05fb6b50757eddf2eabcc5232d77dbf5e"},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
